{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab - Unregularized Logistic Regression\n",
    "\n",
    "**Objectives**: Implement Unregularized Logistic Regression and get to see it works on data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** Build a Logistic Regression model to predict whether a student gets admitted into a university. Suppose that you are the administrator of a university department and you want to determine each applicant's chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for Logistic Regression. For each training example, you have the applicant's scores on two exams and the admissions decision. \n",
    "\n",
    "Your task is to build a classification model that estimates an applicant's probability of admission based on the scores from those two exams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "#Import relevant libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "The file *ex2data1.txt* contains the dataset for this problem. The 1st and the 2nd columns are the scores from the exams (X), the 3rd column (y) indicates if the student was admitted (1) or not admitted (0). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 examples of the dataset:\n",
      "           0          1  2\n",
      "0  34.623660  78.024693  0\n",
      "1  30.286711  43.894998  0\n",
      "2  35.847409  72.902198  0\n",
      "3  60.182599  86.308552  1\n",
      "4  79.032736  75.344376  1\n",
      "\n",
      "Shape of X (features): (100, 2)\n",
      "Shape of y (labels): (100,)\n",
      "Number of training examples (m): 100\n",
      "Number of features (n): 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (file has no header, so we use header=None)\n",
    "data = pd.read_csv('ex2data1.txt', header=None)\n",
    "\n",
    "# Extract the matrix of features (first two columns)\n",
    "X = data.iloc[:, 0:2].values\n",
    "\n",
    "# Extract the labels (third column)\n",
    "y = data.iloc[:, 2].values\n",
    "\n",
    "# Check the shape of X and y\n",
    "m = X.shape[0]  # number of training examples\n",
    "n = X.shape[1]  # number of features\n",
    "\n",
    "# Output the dataset information\n",
    "print(\"First 5 examples of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "print(f\"\\nShape of X (features): {X.shape}\")\n",
    "print(f\"Shape of y (labels): {y.shape}\")\n",
    "print(f\"Number of training examples (m): {m}\")\n",
    "print(f\"Number of features (n): {n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a few examples from Dataframe df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "\n",
    "#df.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data\n",
    "Create a scatter plot of data similar to Fig.1 (using plt.scatter). Students with higher test score for both exam were admitted into the university as expected.\n",
    "\n",
    "<img src=\"images/f1.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> Fig. 1: file ex2data1.txt </center></caption>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m pos\u001b[38;5;241m=\u001b[39m(y\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m neg\u001b[38;5;241m=\u001b[39m(y\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(X[\u001b[43mpos\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m,\u001b[38;5;241m0\u001b[39m],X[pos[:,\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m],c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m,marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(X[neg[:,\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m0\u001b[39m],X[neg[:,\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m],c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m,marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#add figure labels\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "pos=(y==1)\n",
    "neg=(y==0)\n",
    "plt.scatter(X[pos[:,0],0],X[pos[:,0],1],c=\"r\",marker=\"+\")\n",
    "plt.scatter(X[neg[:,0],0],X[neg[:,0],1],c=\"b\",marker=\"o\")\n",
    "\n",
    "#add figure labels\n",
    "plt.xlabel('Feature 1')  # X-axis label\n",
    "plt.ylabel('Feature 2')  # Y-axis label\n",
    "plt.title('Scatter Plot of Data')  # Title of the plot\n",
    "plt.legend()  # Display legend\n",
    "plt.grid(True)  # Optionally add a grid\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function\n",
    "\n",
    "Complete *sigmoid* function that computes $ g(z) = \\frac{1}{(1+e^{-z})}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    return the sigmoid of z\n",
    "    \"\"\"\n",
    "    \n",
    "    gz=?\n",
    "    \n",
    "    return gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the sigmoid function for z=0 => ANSWER =0.5 \n",
    " ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Cost Function and Gradient\n",
    "\n",
    "Recall that the Logistic Regression model is defined as:    $h_{\\theta}(x^{(i)})=  \\frac{1}{1+e^{-\\theta (x^{(i)})}}$\n",
    "\n",
    "The cost function in Logistic Regression is: $J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [ -y^{(i)}log(h_{\\theta}(x^{(i)})) - (1 - y^{(i)})log(1 - (h_{\\theta}(x^{(i)}))]$\n",
    "\n",
    "The gradient of $J(\\theta)$ is a vector of the same length as $\\theta$  where the jth element (for j = 0, 1,…. n) is defined as:\n",
    "$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$\n",
    "\n",
    "Complete function *costFunction* to return $J(\\theta)$ and the gradient ((partial derivative of $J(\\theta)$ with respect to each $\\theta$) for logistic regression. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(theta, X, y):\n",
    "    \"\"\"\n",
    "    Takes in numpy array theta, x and y and return the logistic regression cost function and gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    #number of training examples \n",
    "    m=?\n",
    "    \n",
    "    # model predictions for all training examples\n",
    "    h = ?\n",
    "         \n",
    "    error = (-y * np.log(h)) - ((1-y)*np.log(1-h))\n",
    "\n",
    "    #cost function\n",
    "    cost = 1/m * sum(error)\n",
    "       \n",
    "    #vector of gradients of all model parameters theta   \n",
    "    grad = 1/m * np.dot(X.transpose(),(h - y))\n",
    "    \n",
    "    return cost[0] , grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Feature normalization\n",
    "\n",
    "Mean normalization:\n",
    "\n",
    "$x_i = \\frac{x_i - \\mu_i}{\\sigma_i}$\n",
    "\n",
    "Complete function **featureNormalization(X)**.\n",
    "\n",
    "• Compute the mean value  $\\mu_i$ of each feature (use function np.mean(X,axis=0)) \n",
    "\n",
    "• compute the standard deviation $\\sigma_i$ of each feature (use function np.std(X,axis=0)) \n",
    "\n",
    "• Apply the equation above.\n",
    "\n",
    "**IMPORTANT:** When normalizing the features, it is important to store the mean value and the standard deviation used for normalization. After optimizing the trainable parameters of the model (thetas), you want to use the model for new examples not seen before.\n",
    "You must first normalize the features of the new examples using the mean and standard deviation previously computed from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalization(X):\n",
    "    \"\"\"\n",
    "    Take in numpy array of X values and return normalize X values,\n",
    "    the mean and standard deviation of each feature\n",
    "    \"\"\"\n",
    "    mean= ?\n",
    "    \n",
    "    std= ?\n",
    "    \n",
    "    X_norm =?\n",
    "    \n",
    "    return X_norm , mean , std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run featureNormalization to normalize X, store the means and stds.\n",
    "\n",
    "Xnorm, X_mean, X_std = ?\n",
    "\n",
    "#After normalizing the features, add an extra column of 1's corresponding to x0 = 1.\n",
    "X1= ?\n",
    "\n",
    "# Inicialize VECTOR initial_theta to be equal to 0 (it is not a scalar !!!)\n",
    "initial_theta = ?\n",
    "\n",
    "#Run costFunction\n",
    "cost, grad= ?\n",
    "\n",
    "print(\"Cost for initial theta is\",round(cost,3) )   # ANSWER: Cost of initial theta is 0.693\n",
    "\n",
    "#ANSWER: Gradient for initial theta: [[-0.1 ] [-0.28122914] [-0.25098615]]\n",
    "print(\"Gradient for initial theta:\",grad)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "Implement gradient descent in the function *gradientDescent*. \n",
    "\n",
    "The loop structure is written, you need to supply the updates for $\\theta$  within each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,alpha,num_iters):\n",
    "    \"\"\"\n",
    "    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps\n",
    "    with learning rate of alpha\n",
    "    \n",
    "    return theta and the list of the cost of theta during each iteration\n",
    "    \"\"\"\n",
    "    #number of training examples\n",
    "    m= ?\n",
    "    \n",
    "    J_history =[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        cost, grad = ?\n",
    "        theta = ?\n",
    "        J_history.append(cost)\n",
    "    \n",
    "    return theta , J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradientDescent with learning rate 0.5 and 400 iterations. \n",
    "\n",
    "theta , J_history = ?\n",
    "\n",
    "print(\"Theta optimized by gradient descent:\",theta)\n",
    "\n",
    "#ANSWER: The cost for the optimized theta: 0.205\n",
    "print(\"The cost for the optimized theta:\",round(J_history[-1],3))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Cost Function \n",
    "Choose 400 iterations. Try different values of the learning  rate = [0.01, 0.1, 0.5, 1]\n",
    "and get plots similar to Fig. 2. \n",
    "\n",
    "<img src=\"images/f6.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig. 2** : **Cost function evolution for varying learning rates ** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=[0.01, 0.1, 0.5, 1]\n",
    "for i in lr:\n",
    "    ?\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the decision boundary\n",
    "   \n",
    "Our model is sigmoid function:  $h_{\\theta}(x)=  \\frac{1}{1+e^{-\\theta^T x}}$\n",
    "\n",
    "If $h_\\theta(x) > 0.5$ => predict class \"1\", that is $\\theta^Tx> 0$ => predict class \"1\"\n",
    "\n",
    "If $h_\\theta(x) < 0.5$ => predict class \"0\", that is $\\theta^Tx< 0$ => predict class \"0\" \n",
    "\n",
    "$\\theta^Tx = 0$  is the decision boundary. \n",
    "\n",
    "In this particular case $\\theta_0 + \\theta_1x_1 + \\theta_2x_2 = 0$ is the decision boundary-   \n",
    "\n",
    "Since, we plot $x_1$ against $x_2$, the boundary line will be the equation $ x_2 = \\frac{-(\\theta_0+\\theta_1x_1)}{\\theta_2}$\n",
    "\n",
    "Plot the data and the decision boundary. You should get a figure similar to Fig.3.\n",
    "\n",
    "<img src=\"images/f2.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> Fig. 3: Training data vs Decision boundary </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Fig.3\n",
    "pos=(y==1)\n",
    "neg=(y==0)\n",
    "plt.scatter(Xnorm[pos[:,0],0],Xnorm[pos[:,0],1],c=\"r\",marker=\"+\")\n",
    "plt.scatter(Xnorm[neg[:,0],0],Xnorm[neg[:,0],1],c=\"b\",marker=\"o\")\n",
    "\n",
    "#Sugestion how to plot the decision boundary (the green line)\n",
    "x_value= np.array([np.min(Xnorm[:,1]),np.max(Xnorm[:,1])])\n",
    "y_value=-(theta[0] +theta[1]*x_value)/theta[2]\n",
    "plt.plot(x_value,y_value, \"g\")\n",
    "\n",
    "plt.xlabel(\"Exam 1 score\")\n",
    "plt.ylabel(\"Exam 2 score\")\n",
    "plt.legend([\"Admitted\",\"Not admitted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "For a student with Exam1 score of 45 and Exam2 score of 85, use the learned model to compute what is the admission probability of this student. The answer is around 77% probability (0.767). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array([45,85])\n",
    "\n",
    "#Normalize x_test\n",
    "\n",
    "x_test = ?\n",
    "\n",
    "#Add one \n",
    "x_test = np.append(np.ones(1),x_test)\n",
    "\n",
    "#Compute the logistic regression prediction (the probability for admission)\n",
    "\n",
    "prob = ?\n",
    "\n",
    "print(\"For a student with scores 45 and 85, we predict an admission probability of\",prob[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy on training set \n",
    "\n",
    "Evaluate how well the learned model predicts on the training set. Complete the function *classifierPredict*. \n",
    "\n",
    "The *classifierPredict* function returns a boolean array with True if the probability of admission into university is more than 0.5 and False otherwise. Taking the sum(p==y) adds up all instances where it correctly predicts the given y values (the labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifierPredict(theta,X):\n",
    "    \"\"\"\n",
    "    input theta and X, compute z (what is z ?) \n",
    "    give back binary output  z > 0\n",
    "    \"\"\"\n",
    "    z = ?\n",
    "    \n",
    "    return z>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=classifierPredict(theta,X)\n",
    "\n",
    "print(\"Train Mean Accuracy:\", (sum(p==y)/len(y)*100)[0],\"%\")  #ANSWER: Train Accuracy: 89 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn library to solve the same problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "y=data_n[:,2]  # has to be  1d array\n",
    "\n",
    "logitN = LogisticRegression()\n",
    "logitN.fit(Xnorm,y)\n",
    "print('Accuracy of log reg classifier')\n",
    "print(logitN.score(Xnorm, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " LogisticRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
