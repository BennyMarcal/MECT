{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab  â€“ Linear Regression\n",
    "## PART 1 Univariable Linear Regression\n",
    "\n",
    "Objectives: Implement linear regression with one variable (feature) and get to see it works on data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the lab work you will implement Univariate Linear Regression to predict profits for a food truck (a large vehicle equipped to cook and sell food). Suppose you are the CEO of a restaurant franchise and consider different cities for opening a new food truck. The chain already has trucks in various cities and you have data for profits and populations from the cities. You would like to use this data to help you select in which city to expand your business.\n",
    "\n",
    "First, import all relevant libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Plot the Data\n",
    "\n",
    "The file *Uni_linear.txt* contains the dataset for the Linear Regression problem. The first column is the population of a city (e.g. the independent variable, the predictor, the feature X) and the second column is the profit of a food truck in that city (the predicted variable, the output, the given answer y). The values are scaled: number of people/10000 and profit in dolars/10000. A negative value for profit indicates a loss. \n",
    "\n",
    "\n",
    "<img src=\"images/f1.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> Fig. 1 : file Uni_linear.txt </center></caption>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data into the variable data (using function pd.read_csv from panda library).\n",
    "\n",
    "data=pd.read_csv(?, ?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a few examples from the dataset \n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some statistics\n",
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Fig. 1  (using plt.scatter)\n",
    "\n",
    "\n",
    "#Add labels : plt.xlabel; plt.ylabel; plt.title\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Cost Function $J(\\theta)$\n",
    "\n",
    "Linear Regression cost function: $J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)} )^2$\n",
    "\n",
    "Linear regression model: $h_\\theta(x)=\\theta^Tx=\\theta_0+\\theta_1x_1$ \n",
    "\n",
    "Complete the function **computeCost(X,y,theta)**.  Variables X and y are not scalar values, X is an array (matrix) with dimension (*mx2*), y is an array (vector) with dimension (*mx1*), *m* rows represent the examples from the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X,y,theta):\n",
    "    \"\"\"\n",
    "   Take the arrays X, y, theta and return cost function J for this theta. \n",
    "\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m= ?\n",
    "    \n",
    "    # Use the vectorized dot product with function np.dot() to compute the linear regression model\n",
    "    h= ?\n",
    "    \n",
    "    #Implement the formula above to compute cost function. \n",
    "    #Use function np.sum() to compute the sum of errors over all examples. \n",
    "    J= ?\n",
    "    \n",
    "\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if function **computeCost()** was correctly implemented. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only the values of the two columns from dataFrame data\n",
    "data_n=data.values \n",
    "\n",
    "# The first column of data_n is the population of a city (feature array X) \n",
    "# the second column is the profit of a food truck in that city (the answer, the output y)\n",
    "\n",
    "# Extract X and y \n",
    "\n",
    "X= ?\n",
    "\n",
    "y= ?\n",
    "\n",
    "#Check the shape of X and y, if they are rank 1 arrays (m,), reshape them to be 2-dimensonal arrays (m,1).  \n",
    "#Each example is stored as a row.  \n",
    "?\n",
    "\n",
    "# To take into account the intercept term theta_0, add an additional first column to X and \n",
    "# set it to all ones (use function np.ones). \n",
    "X=np.append(?,?) \n",
    "\n",
    "#Initialize vector theta to 0 (use function np.zeros)\n",
    "theta=\n",
    "\n",
    "\n",
    "# Call function computeCost()\n",
    "?\n",
    "\n",
    "# ANSWER: You should see a cost of about 32.07."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Minimize the cost function $J(\\theta)$ by updating Equation        \n",
    "$\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$ (simultaneously update $\\theta_j$ for all $j$) and repeat unitil convergence. \n",
    "\n",
    "\n",
    "Complete function **gradientDescent**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,alpha,num_iters):\n",
    "    \"\"\"\n",
    "    Take arrays X, y and theta and update theta over num_iters steps with learning rate alpha\n",
    "    \n",
    "     Return: final vector theta and the values of the cost function for each iteration (J_history) \n",
    "    \"\"\"\n",
    "    \n",
    "    m=len(y)\n",
    "    J_history=[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        #compute the linear regression model\n",
    "        h = ?\n",
    "        \n",
    "        #Vectorized way to compute all gradients simultaneously \n",
    "        grad = np.dot(X.transpose(),(h-y))/m \n",
    "        \n",
    "        # updates of vector theta for each iteration\n",
    "        theta= ?\n",
    "        \n",
    "        J_history.append(computeCost(X,y,theta))\n",
    "    \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run  **gradientDescent** with learning rate alpha= 0.01 and 1500 iterations and get the final parameters $\\theta_0$ = -3.630, $\\theta_1$=1.166. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta,J_history = ?\n",
    "\n",
    "#The final Linear Regression model with optimized parameters theta\n",
    "print(\"h(x) =\"+str(round(theta[0,0],2))+\" + \"+str(round(theta[1,0],2))+\"x1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the Cost Function $J(\\theta)$ \n",
    "To understand the cost function $J(\\theta)$ better, we plot the cost in a 3D graph over a grid of values for $\\theta_0$ and $\\theta_1$. The cost function has a global minimum. This minimum is the optimal point for $\\theta_0$ and $\\theta_1$ and each step of gradient descent moves closer to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating values for theta0, theta1 and the resulting cost value\n",
    "theta0_vals=np.linspace(-10,10,100)\n",
    "theta1_vals=np.linspace(-1,4,100)\n",
    "J_vals=np.zeros((len(theta0_vals),len(theta1_vals)))\n",
    "\n",
    "for i in range(len(theta0_vals)):\n",
    "    for j in range(len(theta1_vals)):\n",
    "        t=np.array([theta0_vals[i],theta1_vals[j]])\n",
    "        J_vals[i,j]=computeCost(X,y,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the surface plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "surf=ax.plot_surface(theta0_vals,theta1_vals,J_vals,cmap=\"coolwarm\")\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_xlabel(\"$\\Theta_0$\")\n",
    "ax.set_ylabel(\"$\\Theta_1$\")\n",
    "ax.set_zlabel(\"$J(\\Theta)$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the implementation\n",
    "\n",
    "A good way to verify that gradient descent optimization is working correctly is to plot $J(\\theta)$ over the iterations. Function **gradientDescent** calls function **computeCost** on every iteration and saves the costs over the iterations. If the algorithm works properly, $J(\\theta)$ should never increase, and should converge to a steady value (as shown in Fig.2). \n",
    "\n",
    "<img src=\"images/f2.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> Fig.2 </center></caption>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the gradient history, use function plt.plot(),  and get a curve similar to Fig.2. \n",
    "\n",
    "?\n",
    "\n",
    "#Add labels : plt.xlabel; plt.ylabel; plt.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph with Best Line Fit \n",
    "\n",
    "Plot data and the best line fit (with the optimized  $\\theta$ values) as shown in Fig.3. \n",
    "\n",
    "<img src=\"images/f3.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> Fig. 3 </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "?\n",
    "\n",
    "#add the best line fit with red colour\n",
    "x_fit=range(25)\n",
    "y_fit=theta[0]+theta[1]*x_fit\n",
    "\n",
    "plt.plot(?,?,?)\n",
    "\n",
    "#Add labels : plt.xlabel; plt.ylabel; plt.title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions  using the optimized $\\theta$ values\n",
    "\n",
    "Complete function **predict()** to compute model predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x,theta):\n",
    "    \"\"\"\n",
    "    Takes array of x and theta and return the predicted value of y \n",
    "    \"\"\"\n",
    "     #compute the linear regression model\n",
    "    h= ?\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run  **predict** to predict profits in areas of 35,000 and 70,000 people. Note that you need to scale the numbers properly !\n",
    "\n",
    "Answer: \n",
    "\n",
    "        For population = 35,000, predicted profit of 4520 USD\n",
    "\n",
    "        For population = 70,000, predicted profit of 45342 USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict1= ?\n",
    "print(\"For population = 35,000, what is the predicted profit in $ ?\")\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict2= ?\n",
    "print(\"For population = 70,000, what is the predicted profit in $ ?\")\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  PART 2 Multivariable Linear Regression\n",
    "\n",
    "**Objectives**: Implement linear regression with multiple variables (features) and get to see it works on data. \n",
    "\n",
    "Now you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\n",
    "\n",
    "The file **Multi_linear.txt** contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house.  \n",
    "\n",
    "Load the data into dataframe data2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2= ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a few examples from the dataset \n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some statistics\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization\n",
    "\n",
    "Note that house sizes are much larger values (about 1000 times) than the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly. \n",
    "To make sure features are on a similar scale apply Mean normalization.\n",
    "\n",
    "$x^{norm}_i = \\frac{x_i - \\mu_i}{\\sigma_i}$\n",
    "\n",
    "Complete function **featureNormalization(X)**:\n",
    "\n",
    "â€¢ Compute the mean value  $\\mu_i$ of each feature (use function np.mean(X,axis=0)) \n",
    "\n",
    "â€¢ compute the standard deviation $\\sigma_i$ of each feature (use function np.std(X,axis=0)) \n",
    "\n",
    "â€¢ Apply the equation above.\n",
    "\n",
    "**Remark:** When normalizing the features, it is important to store the mean value and the standard deviation used for normalization. After optimizing the trainable parameters of the model (thetas), you want to predict the price of a new example not seen before.\n",
    "You must first normalize the features of that new example using the mean and standard deviation previously computed from the training set.\n",
    "\n",
    "**Remark:** Mean normalization is an alternative to normalizing by making the absolute values < 1 (i.e. dividing by MaxValue-MinValue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalization(X):\n",
    "    \"\"\"\n",
    "    Take in numpy array of X values and return normalize X values,\n",
    "    the mean and standard deviation of each feature\n",
    "    \"\"\"\n",
    "    #Compute the mean value \n",
    "    mean=?\n",
    "    \n",
    "    # compute the standard deviation  \n",
    "    std=?\n",
    "    \n",
    "    # apply Mean normalization\n",
    "    X_norm = ?\n",
    "    \n",
    "    return X_norm , mean , std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract from data2 the features in X2 and the output in y2. If rank 1, reshape them to have 2 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n2=data2.values\n",
    "\n",
    "#Extract from data2 the features in X2 and the output in y2\n",
    "X2 =?\n",
    "y2=?\n",
    "\n",
    "#Check their shapes, if rank 1, reshape to have 2 dimensions \n",
    "?\n",
    "\n",
    "#Run featureNormalization to normalize X2, store the means and stds.\n",
    "\n",
    "X2, mean_X2, std_X2 = ?\n",
    "\n",
    "#After normalizing the features add an extra column of 1's corresponding to x0 = 1. \n",
    "\n",
    "X2=np.append(.....)\n",
    "\n",
    "\n",
    "#Inicialize the vector of model parameters theta2 to zeros\n",
    "theta2= ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cost $J(\\theta)$\n",
    "In the previous (univariate) problem you implemented functions **computeCost()** and **gradientDescent()** in a vectorized way, therefore they will work for linear regression with any number of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call function computeCost() for the present data\n",
    "\n",
    "?\n",
    "\n",
    "# Answer: Cost = 65591548106.46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Apply gradientDescent with different learning rates (e.g. alpha=[0.001, 0.01, 0.1, 0.3 1.4]) and 400 iterations.\n",
    "\n",
    "If the learning rate is too small (e.g. 0.001), the gradient descent takes a very long time to converge to the optimal value. \n",
    "\n",
    "If the learning rate is too large (e.g. 1.4), $J(\\theta)$ can diverge and \"blow up\", resulting in values which are too large for computer calculations. In these situations, Python will return nan (not a number). This is often caused by undefined operations that involve +/- infinity.\n",
    "\n",
    "Get a similar plot as in Fig.4. \n",
    "\n",
    "<img src=\"images/f4.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> Fig. 4 </center></caption>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicialize vector theta2 to zeros\n",
    "initial_theta2= ?\n",
    "\n",
    "# Apply gradientDescent with different learning rates (e.g. alpha=[0.001, 0.01, 0.1, 0.3 1.4]) and 400 iterations.\n",
    "#You may need to use cicle for. \n",
    "\n",
    "theta2, J_history2 = ?\n",
    "\n",
    "#The final Linear Regression model with optimized parameters theta\n",
    "print(\"h(x) =\"+str(round(theta2[0,0],2))+\" + \"+str(round(theta2[1,0],2))+\"x1 + \"+str(round(theta2[2,0],2))+\"x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions using the optimized $\\theta$ values\n",
    "\n",
    "Using the best learning rate you found, run gradient descent until convergence to find the optimal $\\theta$ values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the price of a house with 1650 square feet and 3 bedrooms  \n",
    "x_sample =?\n",
    "\n",
    "#feature normalisation of x_sample\n",
    "x_sample= ?\n",
    "\n",
    "#add 1\n",
    "x_sample=\n",
    "\n",
    "\n",
    "predict3=predict(?,?)\n",
    "\n",
    "print(\"For size of house = 1650, Number of bedroom = 3, what is the predicted house price in of $ ?\")\n",
    "\n",
    "# Answer: the price is about $293000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
